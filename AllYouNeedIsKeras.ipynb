{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FloppyDoval/Background-Generator/blob/main/AllYouNeedIsKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQTxJa4EaW08"
      },
      "source": [
        "# **IMPORTING DATASET FROM KAGGLE: WMT 2014 English-German**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0s032EDtfWdn",
        "outputId": "fcfaf506-49f4-4b9b-c820-c8124458168e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3o3nC-nSJUd",
        "outputId": "7f4671cd-b656-4593-ba58-a2b0268ab6f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/wmt-2014-english-german\n",
            "/kaggle/input/wmt-2014-english-german/wmt14_translate_de-en_validation.csv\n",
            "/kaggle/input/wmt-2014-english-german/wmt14_translate_de-en_test.csv\n",
            "/kaggle/input/wmt-2014-english-german/wmt14_translate_de-en_train.csv\n"
          ]
        }
      ],
      "source": [
        "# Requirements.txt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "# from tqdm import tqdm\n",
        "import kagglehub\n",
        "\n",
        "# Download from Kaggle\n",
        "path = kagglehub.dataset_download(\"mohamedlotfy50/wmt-2014-english-german\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X80Ur-ibb3um"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_TOqILJRjG0",
        "outputId": "75d672ed-7f99-459e-b086-e082ceb3322b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dataset reduction to 200,000 rows...\n",
            "Start of sampling from /wmt14_translate_de-en_train.csv\n",
            "Initial reservoir filled with 200000 items\n",
            "Processed 1,000,000 lines...\n",
            "Processed 2,000,000 lines...\n",
            "Processed 3,000,000 lines...\n",
            "Processed 4,000,000 lines...\n",
            "Writing 200000 lines to /wmt14_reduced_200k.csv...\n",
            "Reservoir sampling completed in 7.57 seconds\n",
            "Reduced dataset saved to /wmt14_reduced_200k.csv\n",
            "\n",
            "Verifying output file: /wmt14_reduced_200k.csv\n",
            "File size: 56.86 MB\n",
            "Total lines (including header): 200313\n",
            "WARNING: Expected 200001 lines but found 200313\n",
            "\n",
            "Sample content:\n",
            "Header: de,en\n",
            "\n",
            "First 3 data rows:\n",
            "1: \"Für schmale Fugen bis zu 5 mm Breite verwenden Sie bevorzugt Dansand® Fugensand, welcher hellen Quarzsand beinhaltet. Dansand® Steinmehl, aus zermahlenem schwarzem Granit, ist bestens geeignet für breitere Fugen bis 20 mm Breite.\",\"The naturally high pH value of this unique mix creates almost desert like conditions within the joints, which inhibits weed growth and germination.\"\n",
            "2: Sehr professinelles Vorgehen und sehr schnelle Beantwortung der Fragen.,Thanks. Great company to do business with.\n",
            "3: \"Übrigens, sieht ein wöchentliches Animationsprogramm für Kinder spezielle Initiativen vor. Die Beteiligungen an die verschiedenen Aktivitäten garantieren gültige Punkte für die Zuweisung der Trophäe \"\"Ospite della Settimana\"\", und jeden Freitagabend gibt es Preisverleihungen.\",\"Coming down for the Valley of the Cismòn we find Siror, Tonadico that leads to Passo Cereda, Fiera di Primiero with the romantic Val Canali, Transacqua, Mezzano with its wild Val Noana, and, at last, the country of Imer, from where you can reach Val Vanoi.\"\n",
            "\n",
            "Output file verification complete.\n",
            "\n",
            "Dataset reduction completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing data, reducing 4.5 million to 200k from WMT EN-DE dataset\n",
        "\n",
        "TRAIN_FILE = '/wmt14_translate_de-en_train.csv'\n",
        "OUTPUT_FILE = '/wmt14_reduced_200k.csv'\n",
        "TARGET_SIZE = 200000\n",
        "\n",
        "#Using Reservoir Random Sampling because of the size of our data n=4.5million\n",
        "#and we want a representative 200k sample.\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "# ensures the sampling results will be the same every time we run the script\n",
        "random.seed(42)\n",
        "\n",
        "def reservoir_sampling(input_file, output_file, target_size):\n",
        "    \"\"\"\n",
        "    Implement reservoir sampling algorithm for very large files.\n",
        "    This is a one-pass algorithm that doesn't require knowing file size before.\n",
        "\n",
        "    Reservoir sampling works by:\n",
        "    1. Keeping the first 'target_size' items\n",
        "    2. For each subsequent item i:\n",
        "       - Generate random number j between 0 and i\n",
        "       - If j < target_size, replace item j in reservoir with current item\n",
        "\n",
        "    This ensures each item has equal probability (target_size/total) of being selected.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to input CSV train file from WMT2014 en-de\n",
        "        output_file (str): Path to save the reduced dataset\n",
        "        target_size (int): Number of rows to sample\n",
        "    \"\"\"\n",
        "    print(f'Start of sampling from {input_file}')\n",
        "    start_time = time.time()\n",
        "\n",
        "    reservoir = []\n",
        "\n",
        "    with open(input_file, 'rb') as f:\n",
        "        # Read header\n",
        "        header = f.readline().decode('utf-8', errors='replace').strip()\n",
        "\n",
        "        # Fill reservoir with first target_size items\n",
        "        for i in range(target_size):\n",
        "            line = f.readline()\n",
        "            if not line:  # End of file\n",
        "                break\n",
        "            reservoir.append(line.decode('utf-8', errors='replace').strip())\n",
        "\n",
        "        print(f\"Initial reservoir filled with {len(reservoir)} items\")\n",
        "\n",
        "        # Process remaining items with decreasing probability\n",
        "        i = target_size\n",
        "        while True:\n",
        "            line = f.readline()\n",
        "            if not line:  # End of file\n",
        "                break\n",
        "\n",
        "            i += 1\n",
        "            if i % 1000000 == 0:\n",
        "                print(f\"Processed {i:,} lines...\")\n",
        "\n",
        "            # With probability target_size/i, replace a random item in the reservoir\n",
        "            j = random.randrange(i)\n",
        "            if j < target_size:\n",
        "                reservoir[j] = line.decode('utf-8', errors='replace').strip()\n",
        "\n",
        "    # Write reservoir to output file\n",
        "    print(f\"Writing {len(reservoir)} lines to {output_file}...\")\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(header + '\\n')\n",
        "        for line in reservoir:\n",
        "            f.write(line + '\\n')\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Reservoir sampling completed in {elapsed:.2f} seconds\")\n",
        "    print(f\"Reduced dataset saved to {output_file}\")\n",
        "\n",
        "def check_output_file(file_path, expected_count=None):\n",
        "    \"\"\"Verify the output file and print some statistics.\"\"\"\n",
        "    print(f\"\\nVerifying output file: {file_path}\")\n",
        "\n",
        "    # Check if file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        print(\"ERROR: Output file does not exist!\")\n",
        "        return\n",
        "\n",
        "    # Check file size\n",
        "    size_bytes = os.path.getsize(file_path)\n",
        "    size_mb = size_bytes / (1024 * 1024)\n",
        "    print(f\"File size: {size_mb:.2f} MB\")\n",
        "\n",
        "    # Count lines\n",
        "    line_count = 0\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        for _ in f:\n",
        "            line_count += 1\n",
        "\n",
        "    print(f\"Total lines (including header): {line_count}\")\n",
        "\n",
        "    if expected_count and line_count != expected_count + 1:  # +1 for header\n",
        "        print(f\"WARNING: Expected {expected_count + 1} lines but found {line_count}\")\n",
        "\n",
        "    # Print a few sample lines\n",
        "    print(\"\\nSample content:\")\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        header = f.readline().strip()\n",
        "        print(f\"Header: {header}\")\n",
        "\n",
        "        print(\"\\nFirst 3 data rows:\")\n",
        "        for i in range(3):\n",
        "            line = f.readline().strip()\n",
        "            print(f\"{i+1}: {line}\")\n",
        "\n",
        "    print(\"\\nOutput file verification complete.\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to execute sampling with error handling.\"\"\"\n",
        "    print(f\"Starting dataset reduction to {TARGET_SIZE:,} rows...\")\n",
        "\n",
        "    try:\n",
        "        reservoir_sampling(TRAIN_FILE, OUTPUT_FILE, TARGET_SIZE)\n",
        "        # Verify the output file\n",
        "        check_output_file(OUTPUT_FILE, TARGET_SIZE)\n",
        "        print(\"\\nDataset reduction completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during dataset reduction: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRNvSjgfanoD"
      },
      "source": [
        "# **PRE-PROCESSING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0ED0GRPaoGc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}